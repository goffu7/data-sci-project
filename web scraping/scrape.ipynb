{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver_path = \"D:/chromedriver-win64/chromedriver-win64/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2401.00001...\n",
      "Scraped 0 papers.\n",
      "Processing 2401.00002...\n",
      "Scraped 1 papers.\n",
      "Processing 2401.00003...\n",
      "Scraped 2 papers.\n",
      "Processing 2401.00004...\n",
      "Scraped 2 papers.\n",
      "Processing 2401.00005...\n"
     ]
    }
   ],
   "source": [
    "# Setup WebDriver\n",
    "service = Service(webdriver_path) \n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n",
    "data = []\n",
    "\n",
    "# Starting date for the loop\n",
    "month = 1\n",
    "year = 24\n",
    "\n",
    "# Starting paper ID\n",
    "paper_id = 1\n",
    "\n",
    "# Counter for \"not found\" pages\n",
    "not_found_count = 0\n",
    "\n",
    "# Wait for elements to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "while len(data) < 2000:\n",
    "    # Format paper ID (YYMM.XXXXXv1)\n",
    "    yymm = f\"{year:02}{month:02}\"\n",
    "    url = f\"https://arxiv.org/html/{yymm}.{paper_id:05}v1\"\n",
    "    print(f\"Processing {yymm}.{paper_id:05}\")\n",
    "\n",
    "    try:\n",
    "        # Navigate to the page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Check for \"not found\" page\n",
    "        if \"Article not found\" in driver.page_source:\n",
    "            not_found_count += 1\n",
    "            if not_found_count == 2:\n",
    "                break\n",
    "            paper_id += 1\n",
    "            continue\n",
    "\n",
    "        not_found_count = 0  # Reset if valid page is found\n",
    "\n",
    "        # Extract title\n",
    "        try:\n",
    "            title_element = wait.until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"ltx_title.ltx_title_document\"))\n",
    "            )\n",
    "            title_text = title_element.text.strip()\n",
    "        except:\n",
    "            title_text = \"\"\n",
    "\n",
    "        # Extract abstract\n",
    "        try:\n",
    "            abstract_element = wait.until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"ltx_abstract\"))\n",
    "            )\n",
    "            abstract_text = abstract_element.text.replace(\"Abstract\\n\", \"\").strip()\n",
    "        except:\n",
    "            abstract_text = \"\"\n",
    "\n",
    "        # Extract keywords\n",
    "        try:\n",
    "            keyword_element = driver.find_element(By.CLASS_NAME, \"ltx_keywords\")\n",
    "            keyword_text = keyword_element.text.replace(\"Keywords: \", \"\").strip()\n",
    "            keywords_array = keyword_text.split(\", \")\n",
    "        except:\n",
    "            keywords_array = []\n",
    "\n",
    "        # Extract authors and countries\n",
    "        try:\n",
    "            contact_elements = driver.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_affiliation\")\n",
    "            author_elements = driver.find_elements(By.CLASS_NAME, \"ltx_personname\")\n",
    "            authors = [element.text.strip() for element in author_elements]\n",
    "            affiliations = [element.text.strip() for element in contact_elements]\n",
    "            countries = [element.text.strip().split()[-1] for element in contact_elements]\n",
    "        except:\n",
    "            countries = []\n",
    "            affiliations = []\n",
    "            authors = []\n",
    "\n",
    "        # Append data if fields are non-empty\n",
    "        if abstract_text:\n",
    "            data.append({\n",
    "                'title': title_text,\n",
    "                'abstract': abstract_text,\n",
    "                'keyword': keywords_array,\n",
    "                'authors': authors,\n",
    "                'affiliations': affiliations,\n",
    "                'country': countries\n",
    "            })\n",
    "\n",
    "        # Increment paper ID\n",
    "        paper_id += 1\n",
    "\n",
    "        # Stop if 1000 entries are collected\n",
    "        if len(data) >= 1000:\n",
    "            break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error with {yymm}.{paper_id:05}: {e}\")\n",
    "        paper_id += 1\n",
    "        continue\n",
    "\n",
    "    # Handle month/year rollover\n",
    "    if not_found_count == 2:\n",
    "        month += 1\n",
    "        not_found_count = 0\n",
    "        if month > 12:\n",
    "            month = 1\n",
    "            year += 1\n",
    "\n",
    "    # Print amount of papers scraped\n",
    "    print(f\"Scraped {len(data)} papers.\")\n",
    "\n",
    "    # Delay to prevent overload\n",
    "    time.sleep(1)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Save data to a CSV\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"arxiv_papers_selenium.csv\", index=False)\n",
    "\n",
    "print(f\"Scraping complete. Collected {len(data)} entries.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
